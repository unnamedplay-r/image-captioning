{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Captioning Model\n",
    "\n",
    "This notebook does everything, hands down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "from IPython.display import clear_output\n",
    "import traceback\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.callbacks import ModelCheckpoint, ProgbarLogger, TensorBoard, CSVLogger\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, LSTM, Embedding, TimeDistributed, \\\n",
    "                         RepeatVector, Merge, Activation, Flatten\n",
    "from keras.models import Model, Sequential\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Viz\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import SVG\n",
    "\n",
    "\n",
    "rseed = 4444 # use this seed for any functions that utilizes randomness\n",
    "EMBEDDING_DIM = 128\n",
    "GLOVE_DIM = 100\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = '../Flickr8k_Dataset' # path to where the images are located\n",
    "CAPTION_PATH = '../Flickr8k_Captions/Flickr8k.token.txt' # Captions\n",
    "TRAIN_PATH = '../Flickr8k_Captions/Flickr_8k.trainImages.txt' # Ids of Images used for training\n",
    "TEST_PATH = '../Flickr8k_Captions/Flickr_8k.testImages.txt' # Ids of images used for testing\n",
    "GLOVE_DIR = '../glove.6B'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# CaptionGenerator Class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This class handles all of the work required to build and train the model. There are some notable things happening here:\n",
    "1. We're adding <start> and <end> tags to each of the captions and reducing the vocabulary size by removing the words that aren't used often.\n",
    "2. There are 3 sequential models within this. A CNN to understand the images, a LSTM to understand the language within the data, and another LSTM that takes in high level features from the previous models to build a prediction of the next word in a given caption and image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CaptionGenerator():\n",
    "\n",
    "    def __init__(self):\n",
    "        # instantiated -> variable_initializer \n",
    "        # maintains the largest caption size and size of the vocabulary\n",
    "        self.max_cap_len = None\n",
    "        self.vocab_size = None\n",
    "        \n",
    "        # dictionarys that map an index to word and vice versa\n",
    "        self.index_word = None\n",
    "        self.word_index = None\n",
    "        \n",
    "        # tracks the number of training and test samples\n",
    "        self.total_samples = None\n",
    "        self.training_samples = None\n",
    "        self.test_samples = None\n",
    "        \n",
    "        # stores the df used to keep track of the samples and if you have the images encoded, that too :)\n",
    "        self.df = None\n",
    "        self.encoded_images = pickle.load( open( \"encoded_images.pkl\", \"rb\" ) )\n",
    "        \n",
    "        # starts the process! good luck!\n",
    "        self.variable_initializer()\n",
    "\n",
    "    def variable_initializer(self):\n",
    "        \"\"\"\n",
    "        This prepares the caption data, and builds the information declared within the class init\n",
    "        \"\"\"\n",
    "        # load the df and clean the pound signs at the end of the images\n",
    "        self.df = pd.read_csv(CAPTION_PATH,\n",
    "                              sep='\\t',\n",
    "                              header=None,\n",
    "                              names=['image', 'caption'])\n",
    "        self.df['image'] = self.df.image.str.replace(r'#\\d$', '')\n",
    "        \n",
    "        train_df = pd.read_csv(TRAIN_PATH,\n",
    "                               header=None,\n",
    "                               names=['image'])\n",
    "        test_df = pd.read_csv(TEST_PATH,\n",
    "                              header=None,\n",
    "                              names=['image'])\n",
    "        \n",
    "        # add 'train' and 'test' labels and add the start and end tags\n",
    "        # then merge the training and test set into a single df\n",
    "        train_df = pd.merge(train_df, self.df, on='image')\n",
    "        train_df['label'] = 'train'\n",
    "        test_df = pd.merge(test_df, self.df, on='image')\n",
    "        test_df['label'] = 'test'\n",
    "        self.df = pd.concat([train_df, test_df])\n",
    "        \n",
    "        # Preprocess the text by:\n",
    "        #  - adding in the start and end tags, \n",
    "        #  - converting everything to lowercase\n",
    "        self.df['caption'] = self.df.caption.apply(lambda cap: '<start> ' + cap + ' <end>')\n",
    "        self.df['caption'] = self.df.caption.str.lower()\n",
    "        self.df['caption'] = self.df.caption.str.replace('#', 'number')\n",
    "        \n",
    "        # shuffle the dataset\n",
    "        self.df = self.df.sample(frac=1, random_state=rseed)\n",
    "        \n",
    "        # add all the captions\n",
    "        caps = []\n",
    "        for row in self.df.iterrows():\n",
    "            caps.append(row[1][1])\n",
    "            \n",
    "        # This builds our vocabulary. We use this to reference what index means what word, and vice versa.\n",
    "        words = [text.split() for text in caps] # flatten to a list containing all words from the captions\n",
    "        word_count = Counter()\n",
    "        for word in words:\n",
    "            word_count.update(word)\n",
    "        \n",
    "        # adds word or removes them from list\n",
    "        unique = set() # prepare a list to add in all words\n",
    "        kill = set() # words to remove\n",
    "        kill.update(['\"', '\\'', '\\(' ])\n",
    "        for word, count in word_count.items():\n",
    "            if count >= 5:\n",
    "                unique.add(word)\n",
    "            else:\n",
    "                kill.add(word)\n",
    "        \n",
    "        # remove all the words below 5\n",
    "        print('Removing all the words below the count threshold')\n",
    "        self.df['caption'] = self.df.caption.apply(lambda x: \" \".join([word for word in x.split() if word not in kill]))\n",
    "\n",
    "        self.vocab_size = len(unique)\n",
    "        self.word_index = {}\n",
    "        self.index_word = {}\n",
    "        for i, word in enumerate(unique):\n",
    "            # be able to look up corresponding index to word and vice versa\n",
    "            self.word_index[word]=i # word  -> index \n",
    "            self.index_word[i]=word # index -> word\n",
    "        \n",
    "\n",
    "        # rebuilds into the new captions\n",
    "        caps = []\n",
    "        for row in self.df.iterrows():\n",
    "            caps.append(row[1][1])\n",
    "        \n",
    "        print('Building statistics for our dataset')\n",
    "        # Determines what the largest caption is\n",
    "        max_len = 0\n",
    "        for caption in caps:\n",
    "            if(len(caption.split()) > max_len): # checks if this caption is larger than the max\n",
    "                max_len = len(caption.split()) # if so, rewrites the maximum\n",
    "        self.max_cap_len = max_len\n",
    "        \n",
    "        # This calculates the total, training and test samples (aka observations).\n",
    "        # This data is used in the data generator, and we remove 1 because when building\n",
    "        # the dataset for training and testing, we always guess UP to the last word.\n",
    "        self.total_samples=0\n",
    "        for text in caps:\n",
    "            self.total_samples += len(text.split()) - 1 # store the amount of data we have\n",
    "        print(\"Total samples :\", self.total_samples)\n",
    "        \n",
    "        self.training_samples = 0\n",
    "        for row in self.df[self.df.label == 'train'].iterrows():\n",
    "            self.training_samples += len(row[1][1].split()) - 1\n",
    "        print(\"Training samples:\", self.training_samples)\n",
    "        \n",
    "        self.test_samples = 0\n",
    "        for row in self.df[self.df.label == 'test'].iterrows():\n",
    "            self.test_samples += len(row[1][1].split()) - 1\n",
    "        print(\"Test samples:\", self.test_samples)\n",
    "        \n",
    "        print(\"Vocabulary size:\", self.vocab_size)\n",
    "        print(\"Maximum caption length:\", self.max_cap_len)\n",
    "        print(\"Variables initialization done!\")\n",
    "\n",
    "    def create_model(self, ret_model=False, include_base=False):\n",
    "        \n",
    "        # Handles the image encoded features\n",
    "        image_model = Sequential()\n",
    "        image_model.add(Dense(EMBEDDING_DIM, input_dim = 4096, activation='relu'))\n",
    "        image_model.add(RepeatVector(self.max_cap_len))\n",
    "\n",
    "        # Initial Embedding of the Language\n",
    "        lang_model = Sequential()\n",
    "        # lang_model.add(Embedding(self.vocab_size, 256, input_length=self.max_cap_len))\n",
    "        lang_model.add(self.build_embedding_layer())\n",
    "        lang_model.add(LSTM(256, return_sequences=True))\n",
    "        lang_model.add(TimeDistributed(Dense(GLOVE_DIM)))\n",
    "\n",
    "        # The final layer\n",
    "        model = Sequential()\n",
    "        model.add(Merge([image_model, lang_model], mode='concat'))\n",
    "        model.add(LSTM(1000, return_sequences=False, dropout=0.2))\n",
    "        model.add(Dense(self.vocab_size, activity_regularizer=keras.regularizers.l2())) \n",
    "        model.add(Activation('softmax'))\n",
    "\n",
    "        print(\"Model created!\")\n",
    "\n",
    "        if(ret_model == True):\n",
    "            return model\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def build_embedding_layer(self):\n",
    "        print('Building embedding layer.')\n",
    "        embeddings_index = {}\n",
    "        with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "\n",
    "        print('Found %s word vectors within glove.' % len(embeddings_index))\n",
    "        \n",
    "        embedding_matrix = np.zeros((self.vocab_size, GLOVE_DIM))\n",
    "        for word, index in self.word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[index] = embedding_vector\n",
    "                \n",
    "        embedding_layer = Embedding(self.vocab_size,\n",
    "                                    GLOVE_DIM,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=self.max_cap_len,\n",
    "                                    trainable=False)\n",
    "        \n",
    "        return embedding_layer\n",
    "\n",
    "    def data_generator(self, data='train', batch_size = 32):\n",
    "        partial_caps = [] # our features, it holds a list of partial captions\n",
    "        next_words = [] # our predictors, holds a list of vectors that is the next word in a partial caption\n",
    "        images = [] # list of numpy arrays of images for each partial caption\n",
    "        batch_count = 0 # maintains the number of batches we've built\n",
    "        samples_built = 0 # maintains track of how many samples within the batch we've built\n",
    "        \n",
    "        print(\"Generating data...\")\n",
    "        \n",
    "        caps = [] # array to store captions\n",
    "        imgs = [] # array to store paths of images\n",
    "        \n",
    "        # get the training data or test data\n",
    "        for row in self.df[self.df.label == data].iterrows():\n",
    "            imgs.append(row[1][0]) # add the images\n",
    "            caps.append(row[1][1]) # add the caption\n",
    "        \n",
    "        # We want this to continually run while our model is training!\n",
    "        # This continuously returns batches \n",
    "        while True: \n",
    "            current_image = ''\n",
    "            \n",
    "            # start cycling through the captions data, each full caption at a time, and when it's done\n",
    "            for index,text in enumerate(caps):\n",
    "                \n",
    "                # make sure we're not reloading the same image constantly\n",
    "                if current_image != imgs[index]:\n",
    "                    # load's the current image associated with the index\n",
    "                    current_image = imgs[index] # self.load_image(DATASET_PATH + '/' + imgs[index])\n",
    "                    img_encoding = self.encoded_images.loc[current_image]\n",
    "                \n",
    "                # cycle through the entire string length of the caption up until the last one, as we'll\n",
    "                # need that for our prediction.\n",
    "                words_in_caption = text.split()\n",
    "                for i in range(len(text.split())-1): \n",
    "                    \n",
    "                    samples_built += 1\n",
    "                    \n",
    "                    # We first build a partial list of words in a caption, where each element of the list is an index \n",
    "                    # to the word. We then append them to a list where we maintain track of what the partial\n",
    "                    # captions are for a particular sample.\n",
    "                    partial = [self.word_index[txt] for txt in words_in_caption[:i+1]] \n",
    "                    partial_caps.append(partial)\n",
    "                    \n",
    "                    # This stores the next word in the partial sequence above. We retrieve the next word\n",
    "                    # in the sequence we're working on right now, then flip it to 1 for it's respective index.\n",
    "                    next_word = np.zeros(self.vocab_size)\n",
    "                    next_word[ self.word_index[ words_in_caption[i+1] ]] = 1\n",
    "                    \n",
    "                    next_words.append(next_word)\n",
    "                    images.append(img_encoding)\n",
    "                    \n",
    "                    # Check if we hit the batch size, and return the features (X) and predictors (y).\n",
    "                    if samples_built >= batch_size:\n",
    "                        \n",
    "                        # prepare data for Neural Net by creating numpy arrays\n",
    "                        next_words = np.asarray(next_words)\n",
    "                        images = np.asarray(images)\n",
    "                        \n",
    "                        # pad the partial captions so that they're of uniform length, where the length is the \n",
    "                        # size of the largest caption.\n",
    "                        partial_caps = pad_sequences(partial_caps, maxlen=self.max_cap_len, padding='post')\n",
    "                        \n",
    "                        batch_count += 1\n",
    "                        if batch_count % 50 == 0:\n",
    "                            with open(\"batch_watch.log\", \"a\") as f:\n",
    "                                f.write(\"Training on Batch #: {}\\n\".format(batch_count))\n",
    "                        \n",
    "                        yield [[images, partial_caps], next_words]\n",
    "                        \n",
    "                        # reset the feature variables\n",
    "                        partial_caps = []\n",
    "                        next_words = []\n",
    "                        images = []\n",
    "                        samples_built = 0\n",
    "    \n",
    "    def load_image(self, path):\n",
    "        # loads images into target size of VGG net\n",
    "        img = image.load_img(path, target_size=(224,224))\n",
    "        x = image.img_to_array(img)\n",
    "        return np.asarray(x)\n",
    "    \n",
    "    def get_word(self,index):\n",
    "        return self.index_word[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing all the words below the count threshold\n",
      "Building statistics for our dataset\n",
      "Total samples : 438393\n",
      "Training samples: 375515\n",
      "Test samples: 62878\n",
      "Vocabulary size: 2751\n",
      "Maximum caption length: 39\n",
      "Variables initialization done!\n"
     ]
    }
   ],
   "source": [
    "caption_gen = CaptionGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# use this to check on the vocabulary\n",
    "# a = Counter()\n",
    "# for i in caption_gen.df.iterrows():\n",
    "#     a.update([word for word in i[1][1].split()])\n",
    "# a\n",
    "## caption_gen.df[caption_gen.df.caption.str.contains(\"\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Encode Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You want to encode your images using the last layer of VGG16 (or any base model) before dumping it into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "encoding_model = VGG16()\n",
    "layer_name = 'fc2'\n",
    "intermediate_layer_model = Model(inputs=encoding_model.input,\n",
    "                                 outputs=encoding_model.get_layer(layer_name).output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This specs the encoding model\n",
    "SVG(model_to_dot(encoding_model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def encode_img(img_path, model=intermediate_layer_model):\n",
    "    img = image.load_img(DATASET_PATH + '/' + img_path, target_size=(224,224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.asarray(img)\n",
    "    img = img[np.newaxis,:,:,:]\n",
    "    return model.predict(img).flatten()[np.newaxis,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "encoded_images = []\n",
    "for index, img_name in enumerate(caption_gen.df.image.unique()):\n",
    "    print('Encoding image:', index)\n",
    "    img = caption_gen.load_image(DATASET_PATH + '/' + img_name)\n",
    "    img = img[np.newaxis,:,:,:]\n",
    "    pred = intermediate_layer_model.predict(img).flatten()\n",
    "    \n",
    "    encoded_images.append([img_name, pred])\n",
    "\n",
    "df = pd.DataFrame(encoded_images, columns=['image', 'encoding'])\n",
    "df = df.sort_values('image')\n",
    "df = df.set_index('image')\n",
    "# df = df.encoding.apply(lambda x: x.flatten()) # this won't be needed\n",
    "df.to_pickle('encoded_images.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This handles the training for the model. If you need to pick up where you left off, you have options of passing in weights and setting the initial epoch to continue the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(cg, model, weight=None, batch_size=32, epochs=10, initial_epoch=0):\n",
    "\n",
    "    if weight != None:\n",
    "        model.load_weights(weight)\n",
    "    \n",
    "    # location of where your model weights will be stored\n",
    "    file_name = './weights_reduced_vocab/glove_weights.{epoch:02d}-{loss:.2f}.hdf5'\n",
    "    \n",
    "    # Callbacks for tracking the model during training\n",
    "    checkpoint = ModelCheckpoint(file_name, monitor='loss', verbose=1, save_best_only=False, mode='min')\n",
    "    tboard = TensorBoard(log_dir='./logs_reduced_vocab', histogram_freq=2,\n",
    "                         write_graph=True, write_images=False)\n",
    "    csv_logger = CSVLogger('keras_reduced_vocab', separator=',', append=True)\n",
    "    callbacks_list = [checkpoint, tboard, csv_logger]\n",
    "    \n",
    "    # Pass in the data generator and train\n",
    "    try:\n",
    "        model.fit_generator(cg.data_generator(batch_size=batch_size, data='train'),\n",
    "                            steps_per_epoch=cg.training_samples/batch_size,\n",
    "                            validation_data=cg.data_generator(batch_size=batch_size, data='test'),\n",
    "                            validation_steps=cg.test_samples/batch_size,\n",
    "                            epochs=epochs,\n",
    "                            verbose=2,\n",
    "                            callbacks=callbacks_list,\n",
    "                            initial_epoch=initial_epoch)\n",
    "    except Exception as e:\n",
    "        # this will catch and log any issues you have during training\n",
    "        with open('error.log', 'a') as f:\n",
    "            f.write(traceback.format_exc())\n",
    "    \n",
    "    # Save your work!\n",
    "    try:\n",
    "        model.save('../Models/WholeModel.h5', overwrite=True)\n",
    "        model.save_weights('../Models/Weights.h5',overwrite=True)\n",
    "    except:\n",
    "        with open('error.log', 'a') as f:\n",
    "            f.write(\"Error in saving model.\")\n",
    "        print(\"Error in saving model.\")\n",
    "    print(\"Training complete...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building embedding layer.\n",
      "Found 400000 word vectors within glove.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:245: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created!\n"
     ]
    }
   ],
   "source": [
    "caption_model = caption_gen.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVG(model_to_dot(caption_model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caption_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name lstm_8/kernel:0 is illegal; using lstm_8/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_8/recurrent_kernel:0 is illegal; using lstm_8/recurrent_kernel_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_8/bias:0 is illegal; using lstm_8/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense_12/kernel:0 is illegal; using dense_12/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense_12/bias:0 is illegal; using dense_12/bias_0 instead.\n",
      "Epoch 21/50Generating data...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(caption_gen, caption_model,\n",
    "            weight='./weights_reduced_vocab/glove_weights.18-7.81.hdf5',\n",
    "            batch_size=64, epochs=50, initial_epoch=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model\n",
    "\n",
    "This contains a list of functions used in testing the captions produced by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_caption(cg, model, image):\n",
    "    \"\"\"\n",
    "    This is a simple predictor where it simply takes the maximum probability from each predictive step.\n",
    "    \"\"\"\n",
    "    start = cg.word_index['<start>']\n",
    "    captions = [start]\n",
    "\n",
    "    # this will cycle through the sequence until we hit the maximum caption length.\n",
    "    while(len(captions) < cg.max_cap_len):\n",
    "        # pad the sequence so that it fits the input\n",
    "        partial_caption = pad_sequences([captions], maxlen=cg.max_cap_len, padding='post')\n",
    "\n",
    "        # pass the image and caption into the model\n",
    "        next_word_pred = model.predict([image, partial_caption])[0]\n",
    "\n",
    "        # get the largest predictor\n",
    "        next_word = np.argsort(next_word_pred)[-1]\n",
    "\n",
    "        captions.append(next_word)\n",
    "    \n",
    "    full_caption = []\n",
    "    for word in captions:\n",
    "        full_caption.append(cg.index_word[word])\n",
    "    \n",
    "    return \" \".join(full_caption[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def beam_search(cg, model, image, beam_size):\n",
    "    \"\"\"\n",
    "    The idea behind beam search here is that we wish to maximize the probability of a given caption (and get more\n",
    "    natural sounding ones!) by doing a bit deeper of search into the tree of captions. Opposed\n",
    "    to doing something exhaustive, we search for paths by taking some (aka beam size) of\n",
    "    the best possible candidates, and continuing to build predictions from the base branch, and now\n",
    "    beamed branches!\n",
    "    \n",
    "    Check out this video for an idea of what beam search does:\n",
    "    https://www.youtube.com/watch?v=UXW6Cs82UKo\n",
    "    \"\"\"\n",
    "    # this is setting the initial sequence to use the start tag, and total probability of the caption be 0\n",
    "    start = [cg.word_index['<start>']]\n",
    "    captions = [[start,0.0]]\n",
    "    \n",
    "    # this will cycle through the sequence until we hit the maximum caption length.\n",
    "    # captions[0][0] is the sequence we're building, it only started with <start> initially\n",
    "    while(len(captions[0][0]) < cg.max_cap_len):\n",
    "        temp_captions = []\n",
    "        for caption, prob in captions:\n",
    "            # pad the sequence so that it fits the input\n",
    "            partial_caption = pad_sequences([caption], maxlen=cg.max_cap_len, padding='post')\n",
    "            next_words_pred = model.predict([image, partial_caption])[0]\n",
    "            \n",
    "            # sort, and get the indicies of highest probable words (the values at the end of the list)\n",
    "            next_words = np.argsort(next_words_pred)[-beam_size:]\n",
    "            \n",
    "            for word in next_words:\n",
    "                # temporarily store the caption, and the previous probability of the caption,\n",
    "                # then append the news word to the caption\n",
    "                new_partial_caption, new_partial_caption_prob = caption[:], prob\n",
    "                \n",
    "                # add the probability to the total\n",
    "                # next_words_pred[word] gets the probability of that word\n",
    "                new_partial_caption.append(word)\n",
    "                \n",
    "                # appends the list new_partial_caption and the associated probability to the list\n",
    "                # new_partial_caption_prob is a scalar\n",
    "                # an example ouput is [[12,8499, 41], 2.34]\n",
    "                new_partial_caption_prob += next_words_pred[word]\n",
    "                temp_captions.append([new_partial_caption,new_partial_caption_prob])\n",
    "        \n",
    "        captions = temp_captions\n",
    "        captions.sort(key = lambda l:l[1])\n",
    "        captions = captions[-beam_size:]\n",
    "\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_caption(captions):\n",
    "    \"\"\"\n",
    "    Used to filter for the best caption given back from the beam search function and turn the caption\n",
    "    into words\n",
    "    \"\"\"\n",
    "    captions.sort(key = lambda l:l[1])\n",
    "    best_caption = captions[-1][0]\n",
    "    return \" \".join([caption_gen.index_word[index] for index in best_caption])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_caption(caption):\n",
    "    \"\"\"\n",
    "    Removes the <start> and <end> tags from a single caption (Expects a string)\n",
    "    \"\"\"\n",
    "    caption_split = caption.split()\n",
    "    \n",
    "    # removes the <start> tag\n",
    "    processed_caption = caption_split[1:]\n",
    "    \n",
    "    try:\n",
    "        # trys getting the first index of the <end> tag, and if it does we're in business!\n",
    "        end_index = processed_caption.index('<end>')\n",
    "        processed_caption = processed_caption[:end_index]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return \" \".join([word for word in processed_caption])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight = 'weights_reduced_vocab/glove_weights.49-7.80.hdf5'\n",
    "test_model = caption_gen.create_model(ret_model=True)\n",
    "test_model.load_weights(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_image1 = '112178718_87270d9b4d.jpg'\n",
    "test_image2 = '667626_18933d713e.jpg' # girl laying on water\n",
    "test_image3 = '23445819_3a458716c1.jpg' # dogs playing on grass\n",
    "test_image4 = '172097782_f0844ec317.jpg'\n",
    "test_image5 = '242064301_a9d12f1754.jpg'\n",
    "test_image6 = '289599470_cc665e2dfb.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = beam_search(caption_gen, test_model, encode_img(test_image1), beam_size=7)\n",
    "caption = process_caption(get_best_caption(pred))\n",
    "caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_model_on_images(caption_gen, test_model, beam_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model_on_images(cg, model, beam_size = 3):    \n",
    "        # this is setting the initial sequence to use the start tag, and total probability of the caption be 0\n",
    "        start = cg.word_index['<start>']\n",
    "        prob_counter = 0.0\n",
    "        captions = [[[start], prob_counter]]\n",
    "\n",
    "        # this will cycle through the sequence until we hit the maximum caption length.\n",
    "        # captions[0][0] is the sequence we're building, it only started with <start> initially\n",
    "        while(len(captions[0][0]) < cg.max_cap_len):\n",
    "            # preparing the temporary storage of the caption we're building?\n",
    "            temp_captions = []\n",
    "            \n",
    "            # go through the captions we have (i don't know how this is built)\n",
    "            for caption in captions:\n",
    "                # pad the sequence so that it fits the input\n",
    "                partial_caption = pad_sequences([caption], maxlen=cg.max_cap_len, padding='post')\n",
    "\n",
    "                # pass the image and caption into the model\n",
    "                next_word_pred = model.predict([image, partial_caption])[0]\n",
    "\n",
    "                # sort, and get the indicies of highest probable words (the values at the end of the list)\n",
    "                # have to understand what 'beam' does. It might be a beam search, but i'm not sure what that is.\n",
    "                next_word = np.argsort(next_word_pred)[-beam_size:]\n",
    "\n",
    "                captions.append(next_word)\n",
    "\n",
    "                # go through the possible words\n",
    "                for word in next_words:\n",
    "                    # temporarily store the caption, and the previous probability of the caption,\n",
    "                    # then append the news word to the caption\n",
    "                    new_partial_caption, new_partial_caption_prob = caption[0][0], caption[1]\n",
    "                    new_partial_caption.append(word)\n",
    "\n",
    "                    # add the probability to the total\n",
    "                    # next_words_pred[word] gets the probability of that word\n",
    "                    new_partial_caption_prob += next_words_pred[word]\n",
    "\n",
    "                    # appends the list new_partial_caption and the associated probability to the list\n",
    "                    # new_partial_caption_prob is a scalar\n",
    "                    # an example ouput is [[12,8499, 41], 2.34]\n",
    "                    temp_captions.append([new_partial_caption, new_partial_caption_prob])\n",
    "\n",
    "            captions = temp_captions\n",
    "            captions.sort(key = lambda l:l[1])\n",
    "            captions = captions[-beam_size:]\n",
    "    \n",
    "    full_caption = []\n",
    "    for word in captions:\n",
    "        full_caption.append(caption_gen.index_word[word])\n",
    "    \n",
    "    return \" \".join(full_caption[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def beam_prediction(cg, model, image, beam_size=3):\n",
    "    \"\"\"\n",
    "    This is a simple predictor where it simply takes the maximum probability from\n",
    "    each predictive step.\n",
    "    \"\"\"\n",
    "    start = cg.word_index['<start>']\n",
    "    captions = [ [[start],0] ]\n",
    "\n",
    "    # this will cycle through the sequence until we hit the maximum caption length.\n",
    "    while(len(captions) < cg.max_cap_len):\n",
    "        \n",
    "        if len(captions) > 3:\n",
    "            # take the top 3 captions\n",
    "            captions = [sorted(captions, key=lambda x: x[1])[-3:]]\n",
    "        \n",
    "        # for each caption, build a prediction\n",
    "        for caption, prob in captions:\n",
    "            \n",
    "            # pad the sequence so that it fits the input\n",
    "            partial_caption = pad_sequences([caption], maxlen=cg.max_cap_len, padding='post')\n",
    "\n",
    "            # pass the image and caption into the model\n",
    "            next_word_pred = model.predict([image, partial_caption])[0]\n",
    "\n",
    "            # get the index of the largest predictors\n",
    "            next_words = np.argsort(next_word_pred)[-beam_size:]\n",
    "            \n",
    "            temp_captions = []\n",
    "            for next_word in next_words:\n",
    "                # store the new temporary caption and new associated prob\n",
    "                temp_caption = [caption + [next_word], prob + next_word_pred[next_word]]\n",
    "                \n",
    "                # store them\n",
    "                temp_captions.append(temp_caption)\n",
    "            \n",
    "            \n",
    "    \n",
    "    full_caption = []\n",
    "    for word in captions:\n",
    "        full_caption.append(cg.index_word[word])\n",
    "    \n",
    "    return \" \".join(full_caption[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_captions(captions):\n",
    "    final_captions = []\n",
    "    captions.sort(key = lambda l:l[1])\n",
    "    \n",
    "    for caption in captions:\n",
    "        text_caption = \" \".join([caption_gen.index_word[index] for index in caption[0]])\n",
    "        final_captions.append([text_caption, caption[1]])\n",
    "        \n",
    "    return final_captions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
