{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Captioning Model\n",
    "\n",
    "This notebook contains an ensembled neural network that produces captions from a given image. There are outlined sections below detailing each of the components to build and use the model where each contains:\n",
    "1. [The Encoding of the Images](#Encode-Images) - Encodes our images into get high level representations of them using a previously trained model.\n",
    "1. [The Caption Generator Class](#CaptionGenerator-Class) - The class that organizes & prepares the data, houses the model, and utility functions for generating the data to train the model.\n",
    "3. [Training the Model](#Training-the-model) - Contains functions to train our model on a batch basis.\n",
    "4. [Testing the Model](#Testing-the-Model) - Contains functions to test the model (greedy functions, and a beam search)\n",
    "  \n",
    "---\n",
    "  \n",
    "### Data\n",
    "The model is trained on the Flickr8k dataset (you can aquire the dataset by requesting it [here](http://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/KCCA.html)). The dataset is composed of 8k images of various dimensions and each of them has multiple captions associated with them. I used the raw form of the tokens as I didn't have much luck using the lemmatized version. The lemmatized ones made it hard to interpret what the caption was. Also, in order to reduce the complexity of the task, I reduced the vocabulary of my model down to about ~2k words.\n",
    "  \n",
    "\n",
    "  \n",
    "### Model\n",
    "The model is composed of 3 seperate models using the Sequential API in Keras:\n",
    "1. **Visual Model** (CNN) - leverages the VGG16 neural architecture to encode our images, which we then feed directly into a dense layer to perform our own training on those given features.\n",
    "2. **Language Model** (LSTM) - leverages [Stanford's GloVe weights](https://nlp.stanford.edu/projects/glove/) (Wikipedia 2014 + Gigaword 5, 100 dimensions) to obtain vectorized representations of our words which are then fed into a LSTM to encode the captions.\n",
    "3. **Semantic Model** (LSTM) - uses a LSTM with regularization that learns from the high level features from the previous models to generate the next word in a given caption.\n",
    "  \n",
    "\n",
    "  \n",
    "### Training the Model\n",
    "The images are fed into a CNN, and the associated caption is fed into a LSTM. The two features sets extracted from the models are then fed into another LSTM. This is where we have the mapping of our images and current language context into a semantic space.\n",
    "\n",
    "(a VGG16 neural architecture that isn't retrained) \n",
    "  \n",
    "---\n",
    "### Acknowledgements\n",
    "- [anuragmishracse](https://github.com/anuragmishracse/caption_generator) (the source of the project, hats off to you my friend)  \n",
    "- [Andrej Karpathy lectures](https://www.youtube.com/playlist?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC) (especially lecture 10)\n",
    "\n",
    "\n",
    "**This project was composed in:**  \n",
    "`python       3.6.2  \n",
    "keras        2.0.4\n",
    "tensorflow   1.1.0` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "from IPython.display import clear_output\n",
    "import traceback\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.callbacks import ModelCheckpoint, ProgbarLogger, TensorBoard, \\\n",
    "                            CSVLogger\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, LSTM, Embedding, \\\n",
    "                         TimeDistributed, RepeatVector, Merge, Activation, \\ \n",
    "                         Flatten\n",
    "from keras.models import Model, Sequential\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Viz\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import SVG\n",
    "\n",
    "\n",
    "rseed = 4444 # use this seed for any functions that utilizes randomness\n",
    "IMAGE_EMBEDDING_DIM = 128\n",
    "GLOVE_DIM = 100\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Where the images are located\n",
    "DATASET_PATH = '../Flickr8k_Dataset'\n",
    "\n",
    "# These are the full captions\n",
    "CAPTION_PATH = '../Flickr8k_Captions/Flickr8k.token.txt'\n",
    "\n",
    "# IDs of Images used for training\n",
    "TRAIN_PATH = '../Flickr8k_Captions/Flickr_8k.trainImages.txt'\n",
    "\n",
    "# IDs of images used for testing\n",
    "TEST_PATH = '../Flickr8k_Captions/Flickr_8k.testImages.txt'\n",
    "\n",
    "# Glove weights used in embedding layer\n",
    "GLOVE_DIR = '../glove.6B'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to encode your images using the last layer of VGG16 (or any base model) before dumping it into the model. The reason we do this is to significantly reduce our training time. If we pushed this into production, we would need to always encode the images coming in before passing them to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoding_model = VGG16()\n",
    "layer_name = 'fc2'\n",
    "intermediate_layer_model = Model(inputs=encoding_model.input,\n",
    "                                 outputs=(encoding_model\n",
    "                                          .get_layer(layer_name).output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This specs the encoding model\n",
    "SVG(model_to_dot(encoding_model, show_shapes=True).create(prog='dot', \n",
    "                                                          format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image(self, path):\n",
    "    # loads images into target size of VGG net\n",
    "    img = image.load_img(path, target_size=(224,224))\n",
    "    x = image.img_to_array(img)\n",
    "    return np.asarray(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_img(img_path, model=intermediate_layer_model):\n",
    "    img = image.load_img(DATASET_PATH + '/' + img_path, target_size=(224,224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.asarray(img)\n",
    "    img = img[np.newaxis,:,:,:]\n",
    "    return model.predict(img).flatten()[np.newaxis,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_images = []\n",
    "for index, img_name in enumerate(caption_gen.df.image.unique()):\n",
    "    print('Encoding image:', index)\n",
    "    img = load_image(DATASET_PATH + '/' + img_name)\n",
    "    img = img[np.newaxis,:,:,:]\n",
    "    pred = intermediate_layer_model.predict(img).flatten()\n",
    "    \n",
    "    encoded_images.append([img_name, pred])\n",
    "\n",
    "df = pd.DataFrame(encoded_images, columns=['image', 'encoding'])\n",
    "df = df.sort_values('image')\n",
    "df = df.set_index('image')\n",
    "df.to_pickle('encoded_images.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CaptionGenerator Class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class handles all of the work required to build and train the model. There are some notable things happening here:\n",
    "1. We're adding <start> and <end> tags to each of the captions and reducing the vocabulary size by removing the words that aren't used often.\n",
    "2. There are 3 sequential models within this. A CNN to understand the images, a LSTM to understand the language within the data, and another LSTM that takes in high level features from the previous models to build a prediction of the next word in a given caption and image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CaptionGenerator():\n",
    "    \"\"\"\n",
    "    This is a wide arching class that has the primary purpose of composing the\n",
    "    data into the right format, building the model, and preparing the data for\n",
    "    testing and training.\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Initialization Functions\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # instantiated -> variable_initializer \n",
    "        # maintains the largest caption size and size of the vocabulary\n",
    "        self.max_cap_len = None\n",
    "        self.vocab_size = None\n",
    "        \n",
    "        # dictionarys that map an index to word and vice versa\n",
    "        self.index_word = None\n",
    "        self.word_index = None\n",
    "        \n",
    "        # tracks the number of training and test samples\n",
    "        self.total_samples = None\n",
    "        self.training_samples = None\n",
    "        self.test_samples = None\n",
    "        \n",
    "        # stores the df used to keep track of the samples and if you have the\n",
    "        # images encoded, that too :)\n",
    "        self.df = None\n",
    "        self.encoded_images = pickle.load(open( \"encoded_images.pkl\", \"rb\" ))\n",
    "        \n",
    "        # starts the process! good luck!\n",
    "        self.variable_initializer()\n",
    "\n",
    "    def variable_initializer(self):\n",
    "        \"\"\"\n",
    "        This prepares the caption data, and builds the information declared\n",
    "        within the class init\n",
    "        \"\"\"\n",
    "        # load the df and clean the pound signs at the end of the images\n",
    "        self.df = pd.read_csv(CAPTION_PATH,\n",
    "                              sep='\\t',\n",
    "                              header=None,\n",
    "                              names=['image', 'caption'])\n",
    "        self.df['image'] = self.df.image.str.replace(r'#\\d$', '')\n",
    "        \n",
    "        train_df = pd.read_csv(TRAIN_PATH,\n",
    "                               header=None,\n",
    "                               names=['image'])\n",
    "        test_df = pd.read_csv(TEST_PATH,\n",
    "                              header=None,\n",
    "                              names=['image'])\n",
    "        \n",
    "        # add 'train' and 'test' labels and add the start and end tags\n",
    "        # then merge the training and test set into a single df\n",
    "        train_df = pd.merge(train_df, self.df, on='image')\n",
    "        train_df['label'] = 'train'\n",
    "        test_df = pd.merge(test_df, self.df, on='image')\n",
    "        test_df['label'] = 'test'\n",
    "        self.df = pd.concat([train_df, test_df])\n",
    "        \n",
    "        # Preprocess the text by:\n",
    "        #  - adding in the start and end tags, \n",
    "        #  - converting everything to lowercase\n",
    "        self.df['caption'] = self.df.caption.apply(lambda cap: '<start> ' +\n",
    "                                                               cap + ' <end>')\n",
    "        self.df['caption'] = self.df.caption.str.lower()\n",
    "        self.df['caption'] = self.df.caption.str.replace('#', 'number')\n",
    "        \n",
    "        # shuffle the dataset\n",
    "        self.df = self.df.sample(frac=1, random_state=rseed)\n",
    "        \n",
    "        # add all the captions\n",
    "        caps = []\n",
    "        for row in self.df.iterrows():\n",
    "            caps.append(row[1][1])\n",
    "            \n",
    "        # This builds our vocabulary. We use this to reference what index \n",
    "        # means what word, and vice versa.\n",
    "        words = [text.split() for text in caps] # flatten all words into a list\n",
    "        word_count = Counter()\n",
    "        for word in words:\n",
    "            word_count.update(word)\n",
    "        \n",
    "        # adds word or removes them from list\n",
    "        unique = set() # prepare a list to add in all words\n",
    "        kill = set() # words to remove\n",
    "        kill.update(['\"', '\\'', '\\(' ])\n",
    "        for word, count in word_count.items():\n",
    "            if count >= 5:\n",
    "                unique.add(word)\n",
    "            else:\n",
    "                kill.add(word)\n",
    "        \n",
    "        # remove all the words below 5\n",
    "        print('Removing all the words below the count threshold')\n",
    "        self.df['caption'] = (self.df.caption\n",
    "                              .apply(lambda x: \" \".join([word \n",
    "                                                         for word in x.split()\n",
    "                                                         if word not in kill])))\n",
    "\n",
    "        self.vocab_size = len(unique)\n",
    "        self.word_index = {}\n",
    "        self.index_word = {}\n",
    "        for i, word in enumerate(unique):\n",
    "            # be able to look up corresponding index to word and vice versa\n",
    "            self.word_index[word]=i # word  -> index \n",
    "            self.index_word[i]=word # index -> word\n",
    "        \n",
    "\n",
    "        # rebuilds into the new captions\n",
    "        caps = []\n",
    "        for row in self.df.iterrows():\n",
    "            caps.append(row[1][1])\n",
    "        \n",
    "        print('Building statistics for our dataset')\n",
    "        # Determines what the largest caption is\n",
    "        max_len = 0\n",
    "        for caption in caps:\n",
    "            # checks if this caption is larger than the max\n",
    "            if(len(caption.split()) > max_len):\n",
    "                max_len = len(caption.split()) # if so, rewrites the maximum\n",
    "        self.max_cap_len = max_len\n",
    "        \n",
    "        # This calculates the total, training and test samples (aka \n",
    "        # observations). This data is used in the data generator, and we \n",
    "        # remove 1 because when building the dataset for training and testing,\n",
    "        # we always guess UP to the last word.\n",
    "        self.total_samples = 0\n",
    "        for text in caps:\n",
    "            # store the amount of data we have\n",
    "            self.total_samples += len(text.split()) - 1\n",
    "        print(\"Total samples :\", self.total_samples)\n",
    "        \n",
    "        self.training_samples = 0\n",
    "        for row in self.df[self.df.label == 'train'].iterrows():\n",
    "            self.training_samples += len(row[1][1].split()) - 1\n",
    "        print(\"Training samples:\", self.training_samples)\n",
    "        \n",
    "        self.test_samples = 0\n",
    "        for row in self.df[self.df.label == 'test'].iterrows():\n",
    "            self.test_samples += len(row[1][1].split()) - 1\n",
    "        print(\"Test samples:\", self.test_samples)\n",
    "        \n",
    "        print(\"Vocabulary size:\", self.vocab_size)\n",
    "        print(\"Maximum caption length:\", self.max_cap_len)\n",
    "        print(\"Variables initialization done!\")\n",
    "    \n",
    "    \"\"\"\n",
    "    Model Production Functions\n",
    "    \"\"\"\n",
    "    def create_model(self, ret_model=False, include_base=False):\n",
    "        \n",
    "        # Handles the image encoded features\n",
    "        image_model = Sequential()\n",
    "        image_model.add(Dense(IMAGE_EMBEDDING_DIM, \n",
    "                              input_dim = 4096,\n",
    "                              activation='relu'))\n",
    "        image_model.add(RepeatVector(self.max_cap_len))\n",
    "\n",
    "        # Initial Embedding of the Language\n",
    "        lang_model = Sequential()\n",
    "        # lang_model.add(Embedding(self.vocab_size, 256, \n",
    "                                  #input_length=self.max_cap_len))\n",
    "        lang_model.add(self.build_embedding_layer())\n",
    "        lang_model.add(LSTM(256, return_sequences=True))\n",
    "        lang_model.add(TimeDistributed(Dense(GLOVE_DIM)))\n",
    "\n",
    "        # The final layer\n",
    "        model = Sequential()\n",
    "        model.add(Merge([image_model, lang_model], mode='concat'))\n",
    "        model.add(LSTM(1000, return_sequences=False, dropout=0.2))\n",
    "        model.add(Dense(self.vocab_size,\n",
    "                        activity_regularizer=keras.regularizers.l2())) \n",
    "        model.add(Activation('softmax'))\n",
    "\n",
    "        print(\"Model created!\")\n",
    "\n",
    "        if(ret_model == True):\n",
    "            return model\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', \n",
    "                      optimizer='adam', \n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def build_embedding_layer(self):\n",
    "        print('Building embedding layer.')\n",
    "        embeddings_index = {}\n",
    "        with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "\n",
    "        print('Found %s word vectors within glove.' % len(embeddings_index))\n",
    "        \n",
    "        embedding_matrix = np.zeros((self.vocab_size, GLOVE_DIM))\n",
    "        for word, index in self.word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[index] = embedding_vector\n",
    "                \n",
    "        embedding_layer = Embedding(self.vocab_size,\n",
    "                                    GLOVE_DIM,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=self.max_cap_len,\n",
    "                                    trainable=False)\n",
    "        \n",
    "        return embedding_layer\n",
    "    \n",
    "    \"\"\"\n",
    "    Training and Testing Functions\n",
    "    \"\"\"\n",
    "    def data_generator(self, data='train', batch_size = 32):\n",
    "        # our features, it holds a list of partial captions\n",
    "        partial_caps = []\n",
    "        \n",
    "        # our predictors, holds a list of vectors that is the next word in a \n",
    "        # partial caption\n",
    "        next_words = []\n",
    "        \n",
    "        # list of numpy arrays of images for each partial caption\n",
    "        images = []\n",
    "        \n",
    "        # maintains the number of batches we've built\n",
    "        batch_count = 0\n",
    "        \n",
    "        # maintains track of how many samples within the batch we've built\n",
    "        samples_built = 0\n",
    "        \n",
    "        print(\"Generating data...\")\n",
    "        \n",
    "        caps = [] # array to store captions\n",
    "        imgs = [] # array to store paths of images\n",
    "        \n",
    "        # get the training data or test data\n",
    "        for row in self.df[self.df.label == data].iterrows():\n",
    "            imgs.append(row[1][0]) # add the images\n",
    "            caps.append(row[1][1]) # add the caption\n",
    "        \n",
    "        # We want this to continually run while our model is training!\n",
    "        # This continuously returns batches \n",
    "        while True: \n",
    "            current_image = ''\n",
    "            \n",
    "            # start cycling through the captions data, each full caption at a\n",
    "            # time, and when it's done\n",
    "            for index,text in enumerate(caps):\n",
    "                \n",
    "                # make sure we're not reloading the same image constantly\n",
    "                if current_image != imgs[index]:\n",
    "                    # load's the current image associated with the index\n",
    "                    current_image = imgs[index]\n",
    "                    img_encoding = self.encoded_images.loc[current_image]\n",
    "                \n",
    "                # cycle through the entire string length of the caption up\n",
    "                # until the last one, as we'll need that for our prediction.\n",
    "                words_in_caption = text.split()\n",
    "                for i in range(len(text.split())-1): \n",
    "                    \n",
    "                    samples_built += 1\n",
    "                    \n",
    "                    # We first build a partial list of words in a caption, \n",
    "                    # where each element of the list is an index to the word.\n",
    "                    # We then append them to a list where we maintain track of\n",
    "                    # what the partial captions are for a particular sample.\n",
    "                    partial = [self.word_index[txt] \n",
    "                               for txt in words_in_caption[:i+1]] \n",
    "                    partial_caps.append(partial)\n",
    "                    \n",
    "                    # This stores the next word in the partial sequence above.\n",
    "                    # We retrieve the next word in the sequence we're working\n",
    "                    # on right now, then flip it to 1 for it's respective index.\n",
    "                    next_word = np.zeros(self.vocab_size)\n",
    "                    next_word[ self.word_index[ words_in_caption[i+1] ]] = 1\n",
    "                    \n",
    "                    next_words.append(next_word)\n",
    "                    images.append(img_encoding)\n",
    "                    \n",
    "                    # Check if we hit the batch size, and return the features\n",
    "                    # (X) and predictors (y).\n",
    "                    if samples_built >= batch_size:\n",
    "                        \n",
    "                        # prepare data for Neural Net by creating numpy arrays\n",
    "                        next_words = np.asarray(next_words)\n",
    "                        images = np.asarray(images)\n",
    "                        \n",
    "                        # pad the partial captions so that they're of uniform \n",
    "                        # length, where the length is the size of the largest\n",
    "                        # caption.\n",
    "                        partial_caps = pad_sequences(partial_caps, \n",
    "                                                     maxlen=self.max_cap_len,\n",
    "                                                     padding='post')\n",
    "                        \n",
    "                        batch_count += 1\n",
    "                        if batch_count % 50 == 0:\n",
    "                            with open(\"batch_watch.log\", \"a\") as f:\n",
    "                                f.write(\"Training on Batch #: {}\\n\".format(batch_count))\n",
    "                        \n",
    "                        yield [[images, partial_caps], next_words]\n",
    "                        \n",
    "                        # reset the feature variables\n",
    "                        partial_caps = []\n",
    "                        next_words = []\n",
    "                        images = []\n",
    "                        samples_built = 0\n",
    "    \n",
    "    \"\"\"\n",
    "    Utility Functions\n",
    "    \"\"\"\n",
    "    def get_word(self,index):\n",
    "        return self.index_word[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing all the words below the count threshold\n",
      "Building statistics for our dataset\n",
      "Total samples : 438393\n",
      "Training samples: 375515\n",
      "Test samples: 62878\n",
      "Vocabulary size: 2751\n",
      "Maximum caption length: 39\n",
      "Variables initialization done!\n"
     ]
    }
   ],
   "source": [
    "caption_gen = CaptionGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use this to check on the vocabulary\n",
    "# a = Counter()\n",
    "# for i in caption_gen.df.iterrows():\n",
    "#     a.update([word for word in i[1][1].split()])\n",
    "# a\n",
    "## caption_gen.df[caption_gen.df.caption.str.contains(\"\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This handles the training for the model. If you need to pick up where you left off, you have options of passing in weights and setting the initial epoch to continue the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(cg, model, weight=None, batch_size=32, epochs=10, initial_epoch=0):\n",
    "\n",
    "    if weight != None:\n",
    "        model.load_weights(weight)\n",
    "    \n",
    "    # location of where your model weights will be stored\n",
    "    file_name = './weights_reduced_vocab/glove_weights.{epoch:02d}-{loss:.2f}.hdf5'\n",
    "    \n",
    "    # Callbacks for tracking the model during training\n",
    "    checkpoint = ModelCheckpoint(file_name, monitor='loss', verbose=1, \n",
    "                                 save_best_only=False, mode='min')\n",
    "    tboard = TensorBoard(log_dir='./logs_reduced_vocab', histogram_freq=2,\n",
    "                         write_graph=True, write_images=False)\n",
    "    csv_logger = CSVLogger('keras_reduced_vocab', separator=',', append=True)\n",
    "    callbacks_list = [checkpoint, tboard, csv_logger]\n",
    "    \n",
    "    # Pass in the data generator and train\n",
    "    try:\n",
    "        model.fit_generator(cg.data_generator(batch_size=batch_size, data='train'),\n",
    "                            steps_per_epoch=cg.training_samples/batch_size,\n",
    "                            validation_data=cg.data_generator(batch_size=batch_size,\n",
    "                                                              data='test'),\n",
    "                            validation_steps=cg.test_samples/batch_size,\n",
    "                            epochs=epochs,\n",
    "                            verbose=2,\n",
    "                            callbacks=callbacks_list,\n",
    "                            initial_epoch=initial_epoch)\n",
    "    except Exception as e:\n",
    "        # this will catch and log any issues you have during training\n",
    "        with open('error.log', 'a') as f:\n",
    "            f.write(traceback.format_exc())\n",
    "    \n",
    "    # Save your work!\n",
    "    try:\n",
    "        model.save('../Models/WholeModel.h5', overwrite=True)\n",
    "        model.save_weights('../Models/Weights.h5',overwrite=True)\n",
    "    except:\n",
    "        with open('error.log', 'a') as f:\n",
    "            f.write(\"Error in saving model.\")\n",
    "        print(\"Error in saving model.\")\n",
    "    print(\"Training complete...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building embedding layer.\n",
      "Found 400000 word vectors within glove.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:245: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created!\n"
     ]
    }
   ],
   "source": [
    "caption_model = caption_gen.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVG(model_to_dot(caption_model, show_shapes=True).create(prog='dot', \n",
    "                                                         format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caption_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name lstm_8/kernel:0 is illegal; using lstm_8/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_8/recurrent_kernel:0 is illegal; using lstm_8/recurrent_kernel_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_8/bias:0 is illegal; using lstm_8/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense_12/kernel:0 is illegal; using dense_12/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense_12/bias:0 is illegal; using dense_12/bias_0 instead.\n",
      "Epoch 21/50Generating data...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(caption_gen, caption_model,\n",
    "            weight='./weights_reduced_vocab/glove_weights.18-7.81.hdf5',\n",
    "            batch_size=64, epochs=50, initial_epoch=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model\n",
    "\n",
    "This contains a list of functions used in testing the captions produced by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_caption(cg, model, image):\n",
    "    \"\"\"\n",
    "    This is a simple predictor where it takes the maximum probability from\n",
    "    each predictive step to generate a caption.\n",
    "    \"\"\"\n",
    "    start = cg.word_index['<start>']\n",
    "    captions = [start]\n",
    "\n",
    "    # this will cycle through the sequence until we hit the maximum caption length.\n",
    "    while(len(captions) < cg.max_cap_len):\n",
    "        # pad the sequence so that it fits the input\n",
    "        partial_caption = pad_sequences([captions], \n",
    "                                        maxlen=cg.max_cap_len,\n",
    "                                        padding='post')\n",
    "\n",
    "        # pass the image and caption into the model\n",
    "        next_word_pred = model.predict([image, partial_caption])[0]\n",
    "\n",
    "        # get the largest predictor\n",
    "        next_word = np.argsort(next_word_pred)[-1]\n",
    "\n",
    "        captions.append(next_word)\n",
    "    \n",
    "    full_caption = []\n",
    "    for word in captions:\n",
    "        full_caption.append(cg.get_word(word))\n",
    "    \n",
    "    return \" \".join(full_caption[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def beam_search(cg, model, image, beam_size):\n",
    "    \"\"\"\n",
    "    The idea behind beam search here is that we wish to maximize the\n",
    "    probability of a given caption (and get more natural sounding ones!) by\n",
    "    doing a bit deeper of search into the tree of captions. Opposed to doing\n",
    "    something exhaustive, we search for paths by taking some (aka beam size)\n",
    "    of the best possible candidates, and continuing to build predictions from\n",
    "    the base branch, and now beamed branches!\n",
    "    \n",
    "    Check out this video for an idea of what beam search does:\n",
    "    https://www.youtube.com/watch?v=UXW6Cs82UKo\n",
    "    \"\"\"\n",
    "    # this is setting the initial sequence to use the start tag, and total \n",
    "    # probability of the caption be 0\n",
    "    start = [cg.word_index['<start>']]\n",
    "    captions = [[start,0.0]]\n",
    "    \n",
    "    # this will cycle through the sequence until we hit the maximum caption\n",
    "    # length. captions[0][0] is the sequence we're building, it only started\n",
    "    # with <start> initially\n",
    "    while(len(captions[0][0]) < cg.max_cap_len):\n",
    "        temp_captions = []\n",
    "        for caption, prob in captions:\n",
    "            # pad the sequence so that it fits the input\n",
    "            partial_caption = pad_sequences([caption],\n",
    "                                            maxlen=cg.max_cap_len,\n",
    "                                            padding='post')\n",
    "            next_words_pred = model.predict([image, partial_caption])[0]\n",
    "            \n",
    "            # sort, and get the indicies of highest probable words (the values\n",
    "            # at the end of the list)\n",
    "            next_words = np.argsort(next_words_pred)[-beam_size:]\n",
    "            \n",
    "            for word in next_words:\n",
    "                # temporarily store the caption, and the previous probability\n",
    "                # of the caption, then append the news word to the caption\n",
    "                new_partial_caption, new_partial_caption_prob = caption[:], prob\n",
    "                \n",
    "                # add the probability to the total\n",
    "                # next_words_pred[word] gets the probability of that word\n",
    "                new_partial_caption.append(word)\n",
    "                \n",
    "                # appends the list new_partial_caption and the associated\n",
    "                # probability to the list. new_partial_caption_prob is a\n",
    "                # scalar. An example ouput is [[12,8499, 41], 2.34]\n",
    "                new_partial_caption_prob += next_words_pred[word]\n",
    "                temp_captions.append([new_partial_caption,new_partial_caption_prob])\n",
    "        \n",
    "        captions = temp_captions\n",
    "        captions.sort(key = lambda l:l[1])\n",
    "        captions = captions[-beam_size:]\n",
    "\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_caption(captions):\n",
    "    \"\"\"(Expects a list containing [[[caption], probability of caption], etc.])\n",
    "    Used to filter for the best caption given back from the beam search\n",
    "    function and turn the caption into words\n",
    "    \"\"\"\n",
    "    captions.sort(key = lambda l:l[1])\n",
    "    best_caption = captions[-1][0]\n",
    "    return \" \".join([caption_gen.get_word(index) for index in best_caption])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_caption(caption):\n",
    "    \"\"\"\n",
    "    Removes the <start> and <end> tags from a single caption\n",
    "    (Expects a string)\n",
    "    \"\"\"\n",
    "    caption_split = caption.split()\n",
    "    \n",
    "    # removes the <start> tag\n",
    "    processed_caption = caption_split[1:]\n",
    "    \n",
    "    try:\n",
    "        # trys getting the first index of the <end> tag, and if it does we're\n",
    "        # in business!\n",
    "        end_index = processed_caption.index('<end>')\n",
    "        processed_caption = processed_caption[:end_index]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return \" \".join([word for word in processed_caption])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight = 'weights_reduced_vocab/glove_weights.49-7.80.hdf5'\n",
    "test_model = caption_gen.create_model(ret_model=True)\n",
    "test_model.load_weights(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_image1 = '112178718_87270d9b4d.jpg'\n",
    "test_image2 = '667626_18933d713e.jpg' # girl laying on water\n",
    "test_image3 = '23445819_3a458716c1.jpg' # dogs playing on grass\n",
    "test_image4 = '172097782_f0844ec317.jpg'\n",
    "test_image5 = '242064301_a9d12f1754.jpg'\n",
    "test_image6 = '289599470_cc665e2dfb.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = beam_search(caption_gen, test_model, encode_img(test_image1), beam_size=7)\n",
    "caption = process_caption(get_best_caption(pred))\n",
    "caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_model_on_images(caption_gen, test_model, beam_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Testing out Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def test_model_on_images(cg, model, beam_size = 3):    \n",
    "        # this is setting the initial sequence to use the start tag, and total probability of the caption be 0\n",
    "        start = cg.word_index['<start>']\n",
    "        prob_counter = 0.0\n",
    "        captions = [[[start], prob_counter]]\n",
    "\n",
    "        # this will cycle through the sequence until we hit the maximum caption length.\n",
    "        # captions[0][0] is the sequence we're building, it only started with <start> initially\n",
    "        while(len(captions[0][0]) < cg.max_cap_len):\n",
    "            # preparing the temporary storage of the caption we're building?\n",
    "            temp_captions = []\n",
    "            \n",
    "            # go through the captions we have (i don't know how this is built)\n",
    "            for caption in captions:\n",
    "                # pad the sequence so that it fits the input\n",
    "                partial_caption = pad_sequences([caption], maxlen=cg.max_cap_len, padding='post')\n",
    "\n",
    "                # pass the image and caption into the model\n",
    "                next_word_pred = model.predict([image, partial_caption])[0]\n",
    "\n",
    "                # sort, and get the indicies of highest probable words (the values at the end of the list)\n",
    "                # have to understand what 'beam' does. It might be a beam search, but i'm not sure what that is.\n",
    "                next_word = np.argsort(next_word_pred)[-beam_size:]\n",
    "\n",
    "                captions.append(next_word)\n",
    "\n",
    "                # go through the possible words\n",
    "                for word in next_words:\n",
    "                    # temporarily store the caption, and the previous probability of the caption,\n",
    "                    # then append the news word to the caption\n",
    "                    new_partial_caption, new_partial_caption_prob = caption[0][0], caption[1]\n",
    "                    new_partial_caption.append(word)\n",
    "\n",
    "                    # add the probability to the total\n",
    "                    # next_words_pred[word] gets the probability of that word\n",
    "                    new_partial_caption_prob += next_words_pred[word]\n",
    "\n",
    "                    # appends the list new_partial_caption and the associated probability to the list\n",
    "                    # new_partial_caption_prob is a scalar\n",
    "                    # an example ouput is [[12,8499, 41], 2.34]\n",
    "                    temp_captions.append([new_partial_caption, new_partial_caption_prob])\n",
    "\n",
    "            captions = temp_captions\n",
    "            captions.sort(key = lambda l:l[1])\n",
    "            captions = captions[-beam_size:]\n",
    "    \n",
    "    full_caption = []\n",
    "    for word in captions:\n",
    "        full_caption.append(caption_gen.index_word[word])\n",
    "    \n",
    "    return \" \".join(full_caption[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def beam_prediction(cg, model, image, beam_size=3):\n",
    "    \"\"\"\n",
    "    This is a simple predictor where it simply takes the maximum probability from\n",
    "    each predictive step.\n",
    "    \"\"\"\n",
    "    start = cg.word_index['<start>']\n",
    "    captions = [ [[start],0] ]\n",
    "\n",
    "    # this will cycle through the sequence until we hit the maximum caption length.\n",
    "    while(len(captions) < cg.max_cap_len):\n",
    "        \n",
    "        if len(captions) > 3:\n",
    "            # take the top 3 captions\n",
    "            captions = [sorted(captions, key=lambda x: x[1])[-3:]]\n",
    "        \n",
    "        # for each caption, build a prediction\n",
    "        for caption, prob in captions:\n",
    "            \n",
    "            # pad the sequence so that it fits the input\n",
    "            partial_caption = pad_sequences([caption], maxlen=cg.max_cap_len, padding='post')\n",
    "\n",
    "            # pass the image and caption into the model\n",
    "            next_word_pred = model.predict([image, partial_caption])[0]\n",
    "\n",
    "            # get the index of the largest predictors\n",
    "            next_words = np.argsort(next_word_pred)[-beam_size:]\n",
    "            \n",
    "            temp_captions = []\n",
    "            for next_word in next_words:\n",
    "                # store the new temporary caption and new associated prob\n",
    "                temp_caption = [caption + [next_word], prob + next_word_pred[next_word]]\n",
    "                \n",
    "                # store them\n",
    "                temp_captions.append(temp_caption)\n",
    "            \n",
    "            \n",
    "    \n",
    "    full_caption = []\n",
    "    for word in captions:\n",
    "        full_caption.append(cg.index_word[word])\n",
    "    \n",
    "    return \" \".join(full_caption[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_all_captions(captions):\n",
    "    final_captions = []\n",
    "    captions.sort(key = lambda l:l[1])\n",
    "    \n",
    "    for caption in captions:\n",
    "        text_caption = \" \".join([caption_gen.index_word[index] for index in caption[0]])\n",
    "        final_captions.append([text_caption, caption[1]])\n",
    "        \n",
    "    return final_captions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "210px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "437px",
    "left": "1px",
    "right": "20px",
    "top": "105px",
    "width": "212px"
   },
   "toc_section_display": "none",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
