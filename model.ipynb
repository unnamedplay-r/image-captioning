{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "from IPython.display import clear_output\n",
    "import traceback\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.callbacks import ModelCheckpoint, ProgbarLogger, TensorBoard, CSVLogger\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, LSTM, Embedding, TimeDistributed, \\\n",
    "                         RepeatVector, Merge, Activation, Flatten\n",
    "from keras.models import Model, Sequential\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Viz\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import SVG\n",
    "\n",
    "\n",
    "rseed = 4444 # use this seed for any functions that utilizes randomness\n",
    "EMBEDDING_DIM = 128 # how many dimensions we're embedding our image\n",
    "GLOVE_DIM = 100\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = '../Flickr8k_Dataset' # path to where the images are located\n",
    "CAPTION_PATH = '../Flickr8k_Captions/Flickr8k.lemma.token.txt' # Lemmatized tokens\n",
    "TRAIN_PATH = '../Flickr8k_Captions/Flickr_8k.trainImages.txt' # Ids of Images used for training\n",
    "TEST_PATH = '../Flickr8k_Captions/Flickr_8k.testImages.txt' # Ids of images used for testing\n",
    "GLOVE_DIR = '../glove.6B'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Class For Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CaptionGenerator():\n",
    "\n",
    "    def __init__(self):\n",
    "        # this is instantiated in variable_initializer where it maintains the largest caption size\n",
    "        self.max_cap_len = None\n",
    "        self.vocab_size = None\n",
    "        self.index_word = None\n",
    "        self.word_index = None\n",
    "        self.total_samples = None\n",
    "        self.training_samples = None\n",
    "        self.test_samples = None\n",
    "        self.df = None\n",
    "        self.encoded_images = pickle.load( open( \"encoded_images.pkl\", \"rb\" ) )\n",
    "        self.variable_initializer()\n",
    "\n",
    "    def variable_initializer(self):\n",
    "        # load the df and clean the pound signs at the end of the images\n",
    "        self.df = pd.read_csv(CAPTION_PATH,\n",
    "                              sep='\\t',\n",
    "                              header=None,\n",
    "                              names=['image', 'caption'])\n",
    "        self.df['image'] = self.df.image.str.replace(r'#\\d$', '')\n",
    "        \n",
    "        train_df = pd.read_csv(TRAIN_PATH,\n",
    "                               header=None,\n",
    "                               names=['image'])\n",
    "        test_df = pd.read_csv(TEST_PATH,\n",
    "                              header=None,\n",
    "                              names=['image'])\n",
    "        \n",
    "        # merge the training and test set into a single df and add the start and end tags\n",
    "        train_df = pd.merge(train_df, self.df, on='image')\n",
    "        train_df['label'] = 'train'\n",
    "        test_df = pd.merge(test_df, self.df, on='image')\n",
    "        test_df['label'] = 'test'\n",
    "        self.df = pd.concat([train_df, test_df])\n",
    "        self.df['caption'] = self.df.caption.apply(lambda cap: '<start> ' + cap + ' <end>')\n",
    "        \n",
    "        # add all the captions\n",
    "        caps = []\n",
    "        for row in self.df.iterrows():\n",
    "            caps.append(row[1][1])\n",
    "        \n",
    "        # This calculates the total, training and test samples (aka observations).\n",
    "        # This data is used in the data generator, and we remove 1 because when building\n",
    "        # the dataset for training and testing, we always guess UP to the last word.\n",
    "        self.total_samples=0\n",
    "        for text in caps:\n",
    "            self.total_samples += len(text.split()) - 1 # store the amount of data we have\n",
    "        print(\"Total samples :\", self.total_samples)\n",
    "        \n",
    "        self.training_samples = 0\n",
    "        for row in self.df[self.df.label == 'train'].iterrows():\n",
    "            self.training_samples += len(row[1][1].split()) - 1\n",
    "        print(\"Training samples:\", self.training_samples)\n",
    "        \n",
    "        self.test_samples = 0\n",
    "        for row in self.df[self.df.label == 'test'].iterrows():\n",
    "            self.test_samples += len(row[1][1].split()) - 1\n",
    "        print(\"Test samples:\", self.test_samples)\n",
    "        \n",
    "        # This builds our vocabulary. We use this to reference what index means what word, and vice versa.\n",
    "        words = [text.split() for text in caps] # flatten to a list containing all words from the captions\n",
    "        unique = set() # prepare a list to add in all words\n",
    "        for word in words:\n",
    "            unique.update(word) # add each word found\n",
    "\n",
    "        self.vocab_size = len(unique)\n",
    "        self.word_index = {}\n",
    "        self.index_word = {}\n",
    "        for i, word in enumerate(unique):\n",
    "            # be able to look up corresponding index to word and vice versa\n",
    "            self.word_index[word]=i # word  -> index \n",
    "            self.index_word[i]=word # index -> word\n",
    "        \n",
    "        # Determines what the largest caption is\n",
    "        max_len = 0\n",
    "        for caption in caps:\n",
    "            if(len(caption.split()) > max_len): # checks if this caption is larger than the max\n",
    "                max_len = len(caption.split()) # if so, rewrites the maximum\n",
    "        self.max_cap_len = max_len\n",
    "        \n",
    "        print(\"Vocabulary size:\", self.vocab_size)\n",
    "        print(\"Maximum caption length:\", self.max_cap_len)\n",
    "        print(\"Variables initialization done!\")\n",
    "\n",
    "    def build_embedding_layer(self):\n",
    "        print('Building embedding layer.')\n",
    "        embeddings_index = {}\n",
    "        with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "\n",
    "        print('Found %s word vectors within glove.' % len(embeddings_index))\n",
    "        \n",
    "        embedding_matrix = np.zeros((self.vocab_size, GLOVE_DIM))\n",
    "        for word, index in self.word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[index] = embedding_vector\n",
    "                \n",
    "        embedding_layer = Embedding(self.vocab_size,\n",
    "                            GLOVE_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=self.max_cap_len,\n",
    "                            trainable=False)\n",
    "        \n",
    "        return embedding_layer\n",
    "        \n",
    "    \n",
    "    def data_generator(self, data='train', batch_size = 32):\n",
    "        partial_caps = [] # our features, it holds a list of partial captions\n",
    "        next_words = [] # our predictors, holds a list of vectors that is the next word in a partial caption\n",
    "        images = [] # list of numpy arrays of images for each partial caption\n",
    "        batch_count = 0 # maintains the number of batches we've built\n",
    "        samples_built = 0 # maintains track of how many samples within the batch we've built\n",
    "        \n",
    "        print(\"Generating data...\")\n",
    "        \n",
    "        caps = [] # array to store captions\n",
    "        imgs = [] # array to store paths of images\n",
    "        \n",
    "        # get the training data or test data\n",
    "        for row in self.df[self.df.label == data].iterrows():\n",
    "            imgs.append(row[1][0]) # add the images\n",
    "            caps.append(row[1][1]) # add the caption\n",
    "        \n",
    "        # We want this to continually run while our model is training!\n",
    "        # This continuously returns batches \n",
    "        while True: \n",
    "            current_image = ''\n",
    "            \n",
    "            # start cycling through the captions data, each full caption at a time, and when it's done\n",
    "            for index,text in enumerate(caps):\n",
    "                \n",
    "                # make sure we're not reloading the same image constantly\n",
    "                if current_image != imgs[index]:\n",
    "                    # load's the current image associated with the index\n",
    "                    current_image = imgs[index] # self.load_image(DATASET_PATH + '/' + imgs[index])\n",
    "                    img_encoding = self.encoded_images.loc[current_image]\n",
    "                \n",
    "                # cycle through the entire string length of the caption up until the last one, as we'll\n",
    "                # need that for our prediction.\n",
    "                words_in_caption = text.split()\n",
    "                for i in range(len(text.split())-1): \n",
    "                    \n",
    "                    samples_built += 1\n",
    "                    \n",
    "                    # We first build a partial list of words in a caption, where each element of the list is an index \n",
    "                    # to the word. We then append them to a list where we maintain track of what the partial\n",
    "                    # captions are for a particular sample.\n",
    "                    partial = [self.word_index[txt] for txt in words_in_caption[:i+1]] \n",
    "                    partial_caps.append(partial)\n",
    "                    \n",
    "                    # This stores the next word in the partial sequence above. We retrieve the next word\n",
    "                    # in the sequence we're working on right now, then flip it to 1 for it's respective index.\n",
    "                    next_word = np.zeros(self.vocab_size)\n",
    "                    next_word[ self.word_index[ words_in_caption[i+1] ]] = 1\n",
    "                    \n",
    "                    next_words.append(next_word)\n",
    "                    images.append(img_encoding)\n",
    "                    \n",
    "                    # Check if we hit the batch size, and return the features (X) and predictors (y).\n",
    "                    if samples_built >= batch_size:\n",
    "                        \n",
    "                        # prepare data for Neural Net by creating numpy arrays\n",
    "                        next_words = np.asarray(next_words)\n",
    "                        images = np.asarray(images)\n",
    "                        \n",
    "                        # pad the partial captions so that they're of uniform length, where the length is the \n",
    "                        # size of the largest caption.\n",
    "                        partial_caps = pad_sequences(partial_caps, maxlen=self.max_cap_len, padding='post')\n",
    "                        \n",
    "                        batch_count += 1\n",
    "                        if batch_count % 50 == 0:\n",
    "                            with open(\"batch_watch.log\", \"a\") as f:\n",
    "                                f.write(\"Training on Batch #: {}\\n\".format(batch_count))\n",
    "                        \n",
    "                        yield [[images, partial_caps], next_words]\n",
    "                        \n",
    "                        # reset the feature variables\n",
    "                        partial_caps = []\n",
    "                        next_words = []\n",
    "                        images = []\n",
    "                        samples_built = 0\n",
    "    \n",
    "    def load_image(self, path):\n",
    "        img = image.load_img(path, target_size=(224,224))\n",
    "        x = image.img_to_array(img)\n",
    "        return np.asarray(x)\n",
    "\n",
    "    def create_model(self, ret_model=False, include_base=False):\n",
    "        \n",
    "        # Handles the image encoded features\n",
    "        image_model = Sequential()\n",
    "        image_model.add(Dense(EMBEDDING_DIM, input_dim = 4096, activation='relu'))\n",
    "        image_model.add(RepeatVector(self.max_cap_len))\n",
    "\n",
    "        # Initial Embedding of the Language\n",
    "        lang_model = Sequential()\n",
    "        # lang_model.add(Embedding(self.vocab_size, 256, input_length=self.max_cap_len))\n",
    "        lang_model.add(self.build_embedding_layer())\n",
    "        lang_model.add(LSTM(256,return_sequences=True))\n",
    "        lang_model.add(TimeDistributed(Dense(EMBEDDING_DIM)))\n",
    "\n",
    "        # The final layer\n",
    "        model = Sequential()\n",
    "        model.add(Merge([image_model, lang_model], mode='concat'))\n",
    "        model.add(LSTM(1000,return_sequences=False, dropout=0.2))\n",
    "        model.add(Dense(self.vocab_size, activity_regularizer=keras.regularizers.l2())) \n",
    "        model.add(Activation('softmax'))\n",
    "\n",
    "        print(\"Model created!\")\n",
    "\n",
    "        if(ret_model == True):\n",
    "            return model\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def get_word(self,index):\n",
    "        return self.index_word[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples : 446798\n",
      "Training samples: 382760\n",
      "Test samples: 64038\n",
      "Vocabulary size: 6804\n",
      "Maximum caption length: 40\n",
      "Variables initialization done!\n"
     ]
    }
   ],
   "source": [
    "caption_gen = CaptionGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(caption_model, to_file='model.png', show_shapes=False, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ExpatError",
     "evalue": "not well-formed (invalid token): line 1, column 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mExpatError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-854512dece13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSVG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dot'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/gabrielruiz/anaconda/lib/python3.6/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename)\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gabrielruiz/anaconda/lib/python3.6/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self, svg)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mminidom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0msvg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast_bytes_py2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminidom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m         \u001b[0;31m# get svg tag (should be 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0mfound_svg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetElementsByTagName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'svg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gabrielruiz/anaconda/lib/python3.6/xml/dom/minidom.py\u001b[0m in \u001b[0;36mparseString\u001b[0;34m(string, parser)\u001b[0m\n\u001b[1;32m   1966\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1967\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexpatbuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1968\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mexpatbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1969\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1970\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpulldom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gabrielruiz/anaconda/lib/python3.6/xml/dom/expatbuilder.py\u001b[0m in \u001b[0;36mparseString\u001b[0;34m(string, namespaces)\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m         \u001b[0mbuilder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExpatBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gabrielruiz/anaconda/lib/python3.6/xml/dom/expatbuilder.py\u001b[0m in \u001b[0;36mparseString\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_subset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParseEscape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mExpatError\u001b[0m: not well-formed (invalid token): line 1, column 0"
     ]
    }
   ],
   "source": [
    "SVG(model_to_dot(caption_model, show_shapes=False).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caption_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Images\n",
    "You want to encode your images using the last layer of VGG16 (or any base model) before dumping it into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoding_model = VGG16()\n",
    "layer_name = 'fc2'\n",
    "intermediate_layer_model = Model(inputs=encoding_model.input,\n",
    "                                 outputs=encoding_model.get_layer(layer_name).output)\n",
    "# intermediate_output = intermediate_layer_model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment this if you want to check out the model\n",
    "# SVG(model_to_dot(encoding_model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_img(img_path, model):\n",
    "    img = image.load_img(DATASET_PATH + '/' + img_path, target_size=(224,224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.asarray(img)\n",
    "    img = img[np.newaxis,:,:,:]\n",
    "    return intermediate_layer_model.predict(img).flatten()[np.newaxis,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_images = []\n",
    "for index, img_name in enumerate(caption_gen.df.image.unique()):\n",
    "    print('Encoding image:', index)\n",
    "    img = caption_gen.load_image(DATASET_PATH + '/' + img_name)\n",
    "    img = img[np.newaxis,:,:,:]\n",
    "    pred = intermediate_layer_model.predict(img).flatten()\n",
    "    \n",
    "    encoded_images.append([img_name, pred])\n",
    "\n",
    "df = pd.DataFrame(encoded_images, columns=['image', 'encoding'])\n",
    "df = df.sort_values('image')\n",
    "df = df.set_index('image')\n",
    "# df = df.encoding.apply(lambda x: x.flatten()) # this won't be needed\n",
    "df.to_pickle('encoded_images.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(cg, model, weight=None, batch_size=32, epochs=10, initial_epoch=0):\n",
    "\n",
    "    if weight != None:\n",
    "        model.load_weights(weight)\n",
    "\n",
    "    counter = 0\n",
    "    file_name = './weights_regularization/glove_weights.{epoch:02d}-{loss:.2f}.hdf5'\n",
    "    \n",
    "    # Callbacks\n",
    "    checkpoint = ModelCheckpoint(file_name, monitor='loss', verbose=1, save_best_only=False, mode='min')\n",
    "    progbar = ProgbarLogger(count_mode='steps')\n",
    "    tboard = TensorBoard(log_dir='./logs_regularization', histogram_freq=2,\n",
    "                         write_graph=True, write_images=False)\n",
    "    csv_logger = CSVLogger('keras_logs_regularization', separator=',', append=True)\n",
    "    callbacks_list = [checkpoint, progbar, tboard, csv_logger]\n",
    "    \n",
    "    # Pass in the data generator and train\n",
    "    try:\n",
    "        model.fit_generator(cg.data_generator(batch_size=batch_size, data='train'),\n",
    "                            steps_per_epoch=cg.training_samples/batch_size,\n",
    "                            validation_data=cg.data_generator(batch_size=batch_size, data='test'),\n",
    "                            validation_steps=cg.test_samples/batch_size,\n",
    "                            epochs=epochs,\n",
    "                            verbose=2,\n",
    "                            callbacks=callbacks_list,\n",
    "                            initial_epoch=initial_epoch)\n",
    "    except Exception as e:\n",
    "        with open('error.log', 'w') as f:\n",
    "            f.write(traceback.format_exc())\n",
    "    \n",
    "    # Save your work!\n",
    "    try:\n",
    "        model.save('../Models/WholeModel.h5', overwrite=True)\n",
    "        model.save_weights('../Models/Weights.h5',overwrite=True)\n",
    "    except:\n",
    "        print(\"Error in saving model.\")\n",
    "    print(\"Training complete...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building embedding layer.\n",
      "Found 400000 word vectors within glove.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrielruiz/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:212: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created!\n"
     ]
    }
   ],
   "source": [
    "caption_model = caption_gen.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(caption_gen, caption_model, batch_size=32, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for testing used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_caption(caption):\n",
    "    caption_split = caption.split()\n",
    "    processed_caption = caption_split[1:]\n",
    "    \n",
    "    try:\n",
    "        end_index = processed_caption.index('<end>')\n",
    "        processed_caption = processed_caption[:end_index]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return \" \".join([word for word in processed_caption])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_caption(captions):\n",
    "    captions.sort(key = lambda l:l[1])\n",
    "    best_caption = captions[-1][0]\n",
    "    print(best_caption)\n",
    "    return \" \".join([caption_gen.index_word[index] for index in best_caption])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_captions(captions):\n",
    "    final_captions = []\n",
    "    captions.sort(key = lambda l:l[1])\n",
    "    \n",
    "    for caption in captions:\n",
    "        text_caption = \" \".join([caption_gen.index_word[index] for index in caption[0]])\n",
    "        final_captions.append([text_caption, caption[1]])\n",
    "        \n",
    "    return final_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_captions(model, image, beam_size):\n",
    "    # this is setting the initial sequence to be the start tag, and 0 ??\n",
    "    start = [cg.word_index['<start>']]\n",
    "    captions = [[start,0.0]]\n",
    "    \n",
    "    while(len(captions[0][0]) < cg.max_cap_len):\n",
    "        temp_captions = []\n",
    "        for caption in captions:\n",
    "            partial_caption = sequence.pad_sequences([caption[0]], maxlen=cg.max_cap_len, padding='post')\n",
    "            next_words_pred = model.predict([np.asarray([image]), np.asarray(partial_caption)])[0]\n",
    "            next_words = np.argsort(next_words_pred)[-beam_size:]\n",
    "            \n",
    "            for word in next_words:\n",
    "                new_partial_caption, new_partial_caption_prob = caption[0][:], caption[1]\n",
    "                new_partial_caption.append(word)\n",
    "                new_partial_caption_prob += next_words_pred[word]\n",
    "                temp_captions.append([new_partial_caption,new_partial_caption_prob])\n",
    "        \n",
    "        captions = temp_captions\n",
    "        captions.sort(key = lambda l:l[1])\n",
    "        captions = captions[-beam_size:]\n",
    "\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(weight, img_name, beam_size = 3):\n",
    "    encoded_images = pickle.load( open( \"encoded_images.p\", \"rb\" ) )\n",
    "    model = cg.create_model(ret_model = True)\n",
    "    model.load_weights(weight)\n",
    "\n",
    "    image = encoded_images[img_name]\n",
    "    captions = generate_captions(model, image, beam_size)\n",
    "    return process_caption(get_best_caption(captions))\n",
    "    #return [process_caption(caption[0]) for caption in get_all_captions(captions)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bleu_score(hypotheses, references):\n",
    "    return nltk.translate.bleu_score.corpus_bleu(references, hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model_on_images(cg, model, beam_size = 3):\n",
    "    # preparing to generate predictions\n",
    "    imgs = cg.df[cg.df.label == 'test'].image\n",
    "#     full = {}\n",
    "    \n",
    "    # logic to load the model and weights if no model was input\n",
    "#     if model:\n",
    "#         print('Using Model')\n",
    "#     else:\n",
    "#         model = cg.create_model(ret_model = True)\n",
    "#         model.load_weights(weight)\n",
    "    \n",
    "    # getting the predictions & best one for each image\n",
    "    for count, img_name in enumerate(imgs[:1]):\n",
    "        print(\"Predicting for image:\", count)\n",
    "        \n",
    "        # load in the image, and add the extra dimension needed for the input to the model\n",
    "        # i think this extra dimension denotes the sample, such that i could pass in multiple samples\n",
    "        # and get multiple outputs?\n",
    "        image = cg.load_image(DATASET_PATH + '/' + img_name)[np.newaxis,:,:,:]\n",
    "        \n",
    "        # this is setting the initial sequence to use the start tag, and total probability of the caption be 0\n",
    "        start = cg.word_index['<start>']\n",
    "#         prob_counter = 0.0\n",
    "#         captions = [[[start], prob_counter]]\n",
    "        captions = [start]\n",
    "\n",
    "        # this will cycle through the sequence until we hit the maximum caption length.\n",
    "        # captions[0][0] is the sequence we're building, it only started with <start> initially\n",
    "        while(len(captions) < cg.max_cap_len):\n",
    "            # preparing the temporary storage of the caption we're building?\n",
    "#             temp_captions = []\n",
    "            \n",
    "            # go through the captions we have (i don't know how this is built)\n",
    "#             for caption in captions:\n",
    "            # pad the sequence so that it fits the input\n",
    "            partial_caption = pad_sequences([captions], maxlen=cg.max_cap_len, padding='post')\n",
    "\n",
    "            # pass the image and caption into the model\n",
    "            next_word_pred = model.predict([image, partial_caption])[0]\n",
    "\n",
    "            # sort, and get the indicies of highest probable words (the values at the end of the list)\n",
    "            # have to understand what 'beam' does. It might be a beam search, but i'm not sure what that is.\n",
    "            next_word = np.argsort(next_word_pred)[-1]#[-beam_size:]\n",
    "            \n",
    "            captions.append(next_word)\n",
    "\n",
    "#                 # go through the possible words\n",
    "#                 for word in next_words:\n",
    "#                     # temporarily store the caption, and the previous probability of the caption,\n",
    "#                     # then append the news word to the caption\n",
    "#                     new_partial_caption, new_partial_caption_prob = caption[0][0], caption[1]\n",
    "#                     new_partial_caption.append(word)\n",
    "                \n",
    "#                     # add the probability to the total\n",
    "#                     # next_words_pred[word] gets the probability of that word\n",
    "#                     new_partial_caption_prob += next_words_pred[word]\n",
    "                    \n",
    "#                     # appends the list new_partial_caption and the associated probability to the list\n",
    "#                     # new_partial_caption_prob is a scalar\n",
    "#                     # an example ouput is [[12,8499, 41], 2.34]\n",
    "#                     temp_captions.append([new_partial_caption, new_partial_caption_prob])\n",
    "            \n",
    "#             captions = temp_captions\n",
    "#             captions.sort(key = lambda l:l[1])\n",
    "#             print(captions)\n",
    "#             captions = captions[-beam_size:]\n",
    "#             print(captions)\n",
    "    \n",
    "    full_caption = []\n",
    "    for word in captions:\n",
    "        full_caption.append(caption_gen.index_word[word])\n",
    "    \n",
    "    return \" \".join(full_caption[1:])\n",
    "        \n",
    "        \n",
    "#         best_caption = process_caption(get_best_caption(image_captions))\n",
    "#         captions[img_name] = best_caption\n",
    "#         print img_name+\" : \"+str(best_caption)\n",
    "#         f_pred_caption.write(img_name+\"\\t\"+str(best_caption))\n",
    "#         f_pred_caption.flush()\n",
    "    \n",
    "    # close the predictions we're making\n",
    "#     f_pred_caption.close()\n",
    "    \n",
    "#     # getting the caption for each image?\n",
    "#     f_captions = open('Flickr8k_text/Flickr8k.token.txt', 'rb')\n",
    "#     captions_text = f_captions.read().strip().split('\\n')\n",
    "#     image_captions_pair = {}\n",
    "#     for row in captions_text:\n",
    "#         row = row.split(\"\\t\")\n",
    "#         row[0] = row[0][:len(row[0])-2]\n",
    "#         try:\n",
    "#             image_captions_pair[row[0]].append(row[1])\n",
    "#         except:\n",
    "#             image_captions_pair[row[0]] = [row[1]]\n",
    "#     f_captions.close()\n",
    "    \n",
    "#     # building the bleu score\n",
    "#     hypotheses=[]\n",
    "#     references = []\n",
    "#     for img_name in imgs:\n",
    "#         hypothesis = captions[img_name]\n",
    "#         reference = image_captions_pair[img_name]\n",
    "#         hypotheses.append(hypothesis)\n",
    "#         references.append(reference)\n",
    "\n",
    "#     return bleu_score(hypotheses, references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_caption(cg, model, image):\n",
    "    start = cg.word_index['<start>']\n",
    "    captions = [start]\n",
    "\n",
    "    # this will cycle through the sequence until we hit the maximum caption length.\n",
    "    while(len(captions) < cg.max_cap_len):\n",
    "        # pad the sequence so that it fits the input\n",
    "        partial_caption = pad_sequences([captions], maxlen=cg.max_cap_len, padding='post')\n",
    "\n",
    "        # pass the image and caption into the model\n",
    "        next_word_pred = model.predict([image, partial_caption])[0]\n",
    "\n",
    "        # get the largest predictor\n",
    "        next_word = np.argsort(next_word_pred)[-1]\n",
    "\n",
    "        captions.append(next_word)\n",
    "    \n",
    "    full_caption = []\n",
    "    for word in captions:\n",
    "        full_caption.append(cg.index_word[word])\n",
    "    \n",
    "    return \" \".join(full_caption[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "61750/(caption_gen.total_samples/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle.dump(caption_gen.word_index, open('word_index.pkl', 'wb'))\n",
    "# pickle(caption_gen.word_index\n",
    "# caption_gen.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight = 'weights/glove_weights.19-5.71.hdf5'\n",
    "test_model = caption_gen.create_model(ret_model=True)\n",
    "test_model.load_weights(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_image = '241374292_11e3198daa.jpg'\n",
    "predict_caption(caption_gen, test_model, encode_img(test_image, test_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_model_on_images(caption_gen, test_model, beam_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# words_in_caption = text.split()\n",
    "# for i in range(len(text.split())-1): # cycle through the entire string length of the caption\n",
    "#     total_count+=1 # i'm assuming this keeps track of the 'windows' you have\n",
    "#     partial = [self.word_index[txt] for txt in text.split()[:i+1]] # this builds the \n",
    "#     partial_caps.append(partial)\n",
    "#     next1 = np.zeros(self.vocab_size)\n",
    "#     next1[ self.word_index[ text.split()[i+1] ]] = 1\n",
    "\n",
    "word_index = {'the':0, 'cat':1, 'ran':2, 'fast':3, 'and':4, \n",
    "              'leapt':5, 'off':6, 'couch':7, 'cow':8, 'dog':9,\n",
    "              'car':10}\n",
    "\n",
    "caps = ['the cat ran fast and leapt off the couch',\n",
    "        'couch and cow and dog car ran and leapt']\n",
    "max_cap_len = 9\n",
    "batch_size = 5\n",
    "vocab_size = len(word_index)\n",
    "total_count = 0\n",
    "partial_caps = list()\n",
    "next_words = list()\n",
    "gen_count = 0\n",
    "\n",
    "# text = each individual caption\n",
    "for text in caps:\n",
    "    words_in_caption = text.split() # split them into words\n",
    "    for i in range(len(words_in_caption) - 1):\n",
    "            total_count += 1\n",
    "            \n",
    "            partial = [word_index[txt] for txt in words_in_caption[:i+1]]\n",
    "#             print(partial)\n",
    "            partial_caps.append(partial)\n",
    "\n",
    "            next1 = np.zeros(vocab_size, dtype=np.int)\n",
    "            next1[ word_index[ words_in_caption[i+1]] ] = 1\n",
    "            print(next1)\n",
    "\n",
    "            next_words.append(next1)\n",
    "\n",
    "            if total_count > batch_size:\n",
    "                partial_caps = pad_sequences(partial_caps, maxlen=max_cap_len, padding='post')\n",
    "                \n",
    "                total_count = 0\n",
    "                gen_count += 1\n",
    "                # pprint(\"yielding count:\", gen_count)\n",
    "                # pprint( [['img',partial_caps], next_words] )\n",
    "                        \n",
    "                partial_caps = []\n",
    "                next_words = []\n",
    "                images = []\n",
    "\n",
    "                # print(partial)\n",
    "# pprint(partial_caps)\n",
    "# pprint(next_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.asarray(next_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(pd.Index(sum([x.split()[:-1] for x in caps], [])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(next_words, columns=list(word_index.keys()), \n",
    "             index=pd.Index(sum([x.split()[:-1] for x in caps], [])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(partial_caps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "next1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caption_gen = CaptionGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keys = list(test1.word_index.keys())\n",
    "test1.word_index[keys[10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keys[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_model = caption_gen.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_model.layers[0].layers[1].layers[0].input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_model = Xception(include_top=False, weights='imagenet')\n",
    "\n",
    "# Create your own input format (here 3x200x200)\n",
    "# input = Input(shape=(3,200,200),name = 'image_input')\n",
    "layer1 = base_model.output\n",
    "layer1 = GlobalAveragePooling2D()(layer1) # add a global spatial average pooling layer\n",
    "layer1 = Dense(1024, activation='relu')(layer1) # let's add a fully-connected layer\n",
    "\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(200, activation='softmax')(layer1)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_arrays_from_file(path):\n",
    "    while 1:\n",
    "        f = open(path)\n",
    "        for line in f:\n",
    "            # create numpy arrays of input data\n",
    "            # and labels, from each line in the file\n",
    "            x, y = process_line(line)\n",
    "            img = load_images(x)\n",
    "            yield (img, y)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train the model on the new data for a few epochs\n",
    "model.fit_generator(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 172 layers and unfreeze the rest:\n",
    "for layer in model.layers[:172]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[172:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model.fit_generator(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
