# Helping The Blind See

This project is aimed to deliver a prototype of an assistive technology tool using image captioning to help the blind understand the visual environment around them. This is done using an encoder-decoder neural network that builds captions word by word from a given image and associated caption.

Take a peek into the [notebook](https://github.com/unnamedplay-r/image-captioning/blob/master/image_captioning_model.ipynb) within the repo. It contains _alllll_ the details for creating, training and testing the model.
